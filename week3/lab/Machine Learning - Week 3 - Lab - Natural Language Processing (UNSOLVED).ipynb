{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Lab - Natural Language Processing\n",
    "## Sentiment Analysis of Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Week 3 ! In today's lab, you will learn about Natural Language Processing (*NLP*). We will compare 3 methods of featurizing text data: \n",
    "* `CountVectorizer` (Bag of Words)\n",
    "* `TfidfVectorizer` (TF-IDF)\n",
    "* `Doc2Vec` \n",
    "\n",
    "in order to perform **sentiment analysis** on the Cornell IMDB movie review corpus (http://www.cs.cornell.edu/people/pabo/movie-review-data/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Format\n",
    "\n",
    "We can't directly input the raw reviews from the Cornell movie review data repository. Instead, we have to \"clean them up\" by:\n",
    "1. Converting everything to lower case\n",
    "2. Removing punctuation\n",
    "3. Removing common words (stop words)\n",
    "4. Stemming\n",
    "\n",
    "'Cleaning up' text is an important **Data Pre-processing** step in NLP, and is crucial to getting good results. In the same way that we do with our numerical features (egs: filling na values with a mean, etc.), we need to make sure that words that we are going to use as features are consistently formatted and don't include information that will end up being unnecessary.\n",
    "\n",
    "To practise, we are going to perform the above 4 steps on the sample movie review below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is nothing short of brilliant. Expertly scripted and perfectly delivered, this searing parody of a students and teachers at a South London Public School leaves you literally rolling with laughter. It's vulgar, provocative, witty and sharp. The characters are a superbly caricatured cross section of British society (or to be more accurate, of any society). Following the escapades of Keisha, Latrina and Natella, our three \"protagonists\" for want of a better term, the show doesn't shy away from parodying every imaginable subject. Political correctness flies out the window in every episode. If you enjoy shows that aren't afraid to poke fun of every taboo subject imaginable, then Bromwell High will not disappoint!\n"
     ]
    }
   ],
   "source": [
    "movie_review = \"\"\"Bromwell High is nothing short of brilliant. Expertly scripted and perfectly delivered, this searing parody of a students and teachers at a South London Public School leaves you literally rolling with laughter. It's vulgar, provocative, witty and sharp. The characters are a superbly caricatured cross section of British society (or to be more accurate, of any society). Following the escapades of Keisha, Latrina and Natella, our three \"protagonists\" for want of a better term, the show doesn't shy away from parodying every imaginable subject. Political correctness flies out the window in every episode. If you enjoy shows that aren't afraid to poke fun of every taboo subject imaginable, then Bromwell High will not disappoint!\"\"\"\n",
    "print(movie_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first step is to lowercase it\n",
    "You can do this using the `.lower()` function. Try it out on `movie_review`, and print it to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_review = movie_review.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to remove punctuation. import `string`, and then from `string` import `punctuation`.\n",
    "Print `punctuation` to see the list of punctuation marks in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we remove punctuation from a string is by creating a `translator` object, and then calling `.translate` on our string using the `translator` object.\n",
    "\n",
    "Create a `translator` object by calling `str.maketrans('', '', punctuation)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran = str.maketrans('', '', punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, call `.translate` on your `movie_review` and pass it your `translator` object. Then, print your movie review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_review = movie_review.translate(tran)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that all the punctuation has been removed!\n",
    "\n",
    "If you want to understand why/how this works, check out these posts:\n",
    "* https://www.tutorialspoint.com/python/string_maketrans.htm\n",
    "* https://stackoverflow.com/questions/34293875/how-to-remove-punctuation-marks-from-a-string-in-python-3-x-using-translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stop words\n",
    "\n",
    "Notice all of the punctuation has been removed.  Next we will remove common words.  This is because in NLP we want to find things that distinct between different sets of texts.  We can make that easier by removing words that are common to ALL texts (and, is are, etc.)\n",
    "\n",
    "from `sklearn.feature_extraction.stop_words` import `ENGLISH_STOP_WORDS`. Then, print `ENGLISH_STOP_WORDS` to see a list of common stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to remove the above words from `movie_review`. First, convert `movie_review` into a list by calling the `.split()` method. Call your new object `split_review`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell',\n",
       " 'high',\n",
       " 'is',\n",
       " 'nothing',\n",
       " 'short',\n",
       " 'of',\n",
       " 'brilliant',\n",
       " 'expertly',\n",
       " 'scripted',\n",
       " 'and',\n",
       " 'perfectly',\n",
       " 'delivered',\n",
       " 'this',\n",
       " 'searing',\n",
       " 'parody',\n",
       " 'of',\n",
       " 'a',\n",
       " 'students',\n",
       " 'and',\n",
       " 'teachers',\n",
       " 'at',\n",
       " 'a',\n",
       " 'south',\n",
       " 'london',\n",
       " 'public',\n",
       " 'school',\n",
       " 'leaves',\n",
       " 'you',\n",
       " 'literally',\n",
       " 'rolling',\n",
       " 'with',\n",
       " 'laughter',\n",
       " 'its',\n",
       " 'vulgar',\n",
       " 'provocative',\n",
       " 'witty',\n",
       " 'and',\n",
       " 'sharp',\n",
       " 'the',\n",
       " 'characters',\n",
       " 'are',\n",
       " 'a',\n",
       " 'superbly',\n",
       " 'caricatured',\n",
       " 'cross',\n",
       " 'section',\n",
       " 'of',\n",
       " 'british',\n",
       " 'society',\n",
       " 'or',\n",
       " 'to',\n",
       " 'be',\n",
       " 'more',\n",
       " 'accurate',\n",
       " 'of',\n",
       " 'any',\n",
       " 'society',\n",
       " 'following',\n",
       " 'the',\n",
       " 'escapades',\n",
       " 'of',\n",
       " 'keisha',\n",
       " 'latrina',\n",
       " 'and',\n",
       " 'natella',\n",
       " 'our',\n",
       " 'three',\n",
       " 'protagonists',\n",
       " 'for',\n",
       " 'want',\n",
       " 'of',\n",
       " 'a',\n",
       " 'better',\n",
       " 'term',\n",
       " 'the',\n",
       " 'show',\n",
       " 'doesnt',\n",
       " 'shy',\n",
       " 'away',\n",
       " 'from',\n",
       " 'parodying',\n",
       " 'every',\n",
       " 'imaginable',\n",
       " 'subject',\n",
       " 'political',\n",
       " 'correctness',\n",
       " 'flies',\n",
       " 'out',\n",
       " 'the',\n",
       " 'window',\n",
       " 'in',\n",
       " 'every',\n",
       " 'episode',\n",
       " 'if',\n",
       " 'you',\n",
       " 'enjoy',\n",
       " 'shows',\n",
       " 'that',\n",
       " 'arent',\n",
       " 'afraid',\n",
       " 'to',\n",
       " 'poke',\n",
       " 'fun',\n",
       " 'of',\n",
       " 'every',\n",
       " 'taboo',\n",
       " 'subject',\n",
       " 'imaginable',\n",
       " 'then',\n",
       " 'bromwell',\n",
       " 'high',\n",
       " 'will',\n",
       " 'not',\n",
       " 'disappoint']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_review = movie_review.split()\n",
    "split_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you want to use a `for` loop to create a new list (call it `clean_words`). In each iteration of your loop, go through `split_review` and check every word. If the word is not in `ENGLISH_STOP_WORDS`, append it to your `clean_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_words = [ word for word in split_review if not word in ENGLISH_STOP_WORDS]\n",
    "len(clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, put the clean words back together to re-create `movie_review`, by using the `.join'` method on `clean_words`, separated by a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high short brilliant expertly scripted perfectly delivered searing parody students teachers south london public school leaves literally rolling laughter vulgar provocative witty sharp characters superbly caricatured cross section british society accurate society following escapades keisha latrina natella protagonists want better term doesnt shy away parodying imaginable subject political correctness flies window episode enjoy shows arent afraid poke fun taboo subject imaginable bromwell high disappoint'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_review = ' '.join(clean_words)\n",
    "movie_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print your movie review! It should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stem words\n",
    "Finally, we will \"stem\" the words so that we take away the differences between words like \"expertly\" and \"expert\" since they have the same meaning. Read more on stemming [here](https://en.wikipedia.org/wiki/Stemming):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `SnowballStemmer` library. import it from `nltk.stem.snowball`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SnowballStemmer takes in a language as an argument. Since we are working with english, create a `SnowballStemmer` object and pass it `english` as the language. Call your object `stemmer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.stem.snowball.SnowballStemmer at 0x7fa46cd115c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the stem of a word, call `stemmer.stem()`. Try it out with the word `running`. See what it prints!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stemmed_word'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('stemmed_words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, similar to how we removed the stop words, we want to now go through our review and stem each word. So:\n",
    "* Turn your `movie_review` back into a list using `.split()`\n",
    "* Create an empty list called `stemmed_words`\n",
    "* Use a `for` loop to go through every word in your `movie_review` and call `stemmer.stem` on it.\n",
    "* Append the newly stemmed word to your `stemmed_words` list\n",
    "* Finally, re-create movie_review into a string by calling `.join` using a space as your separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_word = lambda word: stemmer.stem(word)\n",
    "# stemmed_words = movie_review.split(\" \").apply(stem_word)\n",
    "stemmed_words = list(map(stem_word, movie_review.split(\" \")))\n",
    "movie_review = ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print your final movie review! It should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwel high short brilliant expert script perfect deliv sear parodi student teacher south london public school leav liter roll laughter vulgar provoc witti sharp charact superbl caricatur cross section british societi accur societi follow escapad keisha latrina natella protagonist want better term doesnt shi away parodi imagin subject polit correct fli window episod enjoy show arent afraid poke fun taboo subject imagin bromwel high disappoint'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put it all together\n",
    "We can put all the steps above together in a function, like this (pseudo-code given):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwel high short brilliant expert script perfect deliv sear parodi student teacher south london public school leav liter roll laughter vulgar provoc witti sharp charact superbl caricatur cross section british societi accur societi follow escapad keisha latrina natella protagonist want better term doesnt shi away parodi imagin subject polit correct fli window episod enjoy show arent afraid poke fun taboo subject imagin bromwel high disappoint'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def clean_text(raw_text):\n",
    "#     initialize empty clean_words list\n",
    "#     make raw_text lower case\n",
    "#     remove punctuation from raw_text\n",
    "#     split_words into a list\n",
    "\n",
    "#     for word in split_words:\n",
    "#         if word not in ENGLISH_STOP_WORDS:\n",
    "#             make stemmed_word\n",
    "#             append stemmed_word to clean_words\n",
    "    \n",
    "#     return ' '.join(clean_words)\n",
    "def clean_text(raw_text):\n",
    "    processing_text = raw_text.lower()\n",
    "    tran = str.maketrans('', '', punctuation)\n",
    "    processing_text = processing_text.translate(tran)\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    clean_words = [ stemmer.stem(word) for word in processing_text.split() if word not in ENGLISH_STOP_WORDS ]\n",
    "    return ' '.join(clean_words)\n",
    "movie_review_test = \"\"\"Bromwell High is nothing short of brilliant. Expertly scripted and perfectly delivered, this searing parody of a students and teachers at a South London Public School leaves you literally rolling with laughter. It's vulgar, provocative, witty and sharp. The characters are a superbly caricatured cross section of British society (or to be more accurate, of any society). Following the escapades of Keisha, Latrina and Natella, our three \"protagonists\" for want of a better term, the show doesn't shy away from parodying every imaginable subject. Political correctness flies out the window in every episode. If you enjoy shows that aren't afraid to poke fun of every taboo subject imaginable, then Bromwell High will not disappoint!\"\"\"\n",
    "clean_text(movie_review_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works. Here is an unclean review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is nothing short of brilliant. Expertly scripted and perfectly delivered, this searing parody of a students and teachers at a South London Public School leaves you literally rolling with laughter. It's vulgar, provocative, witty and sharp. The characters are a superbly caricatured cross section of British society (or to be more accurate, of any society). Following the escapades of Keisha, Latrina and Natella, our three \"protagonists\" for want of a better term, the show doesn't shy away from parodying every imaginable subject. Political correctness flies out the window in every episode. If you enjoy shows that aren't afraid to poke fun of every taboo subject imaginable, then Bromwell High will not disappoint!\n"
     ]
    }
   ],
   "source": [
    "unclean_review = \"\"\"Bromwell High is nothing short of brilliant. Expertly scripted and perfectly delivered, this searing parody of a students and teachers at a South London Public School leaves you literally rolling with laughter. It's vulgar, provocative, witty and sharp. The characters are a superbly caricatured cross section of British society (or to be more accurate, of any society). Following the escapades of Keisha, Latrina and Natella, our three \"protagonists\" for want of a better term, the show doesn't shy away from parodying every imaginable subject. Political correctness flies out the window in every episode. If you enjoy shows that aren't afraid to poke fun of every taboo subject imaginable, then Bromwell High will not disappoint!\"\"\"\n",
    "print(unclean_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now clean it by calling the `clean_text` function above and make sure you notice the difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwel high short brilliant expert script perfect deliv sear parodi student teacher south london public school leav liter roll laughter vulgar provoc witti sharp charact superbl caricatur cross section british societi accur societi follow escapad keisha latrina natella protagonist want better term doesnt shi away parodi imagin subject polit correct fli window episod enjoy show arent afraid poke fun taboo subject imagin bromwel high disappoint'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(unclean_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's now clean up all our data!\n",
    "\n",
    "Our data can be be found in the file `all_reviews_small.csv` (some of the cleaning steps have already been done)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import `pandas` and read `all_reviews_small.csv` into a dataframe. Call it `df_reviews`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>train_test_split</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>train</td>\n",
       "      <td>bromwell high is a cartoon comedy it ran at th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>train</td>\n",
       "      <td>homelessness or houselessness as george carlin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>train</td>\n",
       "      <td>brilliant over acting by lesley ann warren bes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>train</td>\n",
       "      <td>this is easily the most underrated film inn th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>train</td>\n",
       "      <td>this is not the typical mel brooks film it was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label train_test_split                                               text\n",
       "0   pos            train  bromwell high is a cartoon comedy it ran at th...\n",
       "1   pos            train  homelessness or houselessness as george carlin...\n",
       "2   pos            train  brilliant over acting by lesley ann warren bes...\n",
       "3   pos            train  this is easily the most underrated film inn th...\n",
       "4   pos            train  this is not the typical mel brooks film it was..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_reviews = pd.read_csv('all_reviews_small.csv')\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the `head` and `shape`. You should see 4000 reviews, with 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['label', 'train_test_split', 'text'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply our `clean_words` function to all the reviews! Store the clean reviews in a new column called `clean_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_reviews['clean_text'] = df_reviews.loc[:, 'text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the `head` again to see your new dataframe's `clean_text` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pos', 'neg'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.head()\n",
    "df_reviews.loc[:, 'label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "In Python, the `CountVectorizer` object represents the Bag Of Words model. import it from `sklearn.feature_extraction.text`, and create a `CountVectorizer()` object called `count_vect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to convert all our `clean_text` reviews into a bag of words representation. Call `count_vect.fit_transform` on all our clean reviews (i.e. `df_review['clean_text']`) to do this. Save the result in `bag_of_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_of_words = count_vect.fit_transform(df_reviews['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print your `bag_of_words`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4000x20719 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 325930 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our vocabulary is so large, CountVectorizer creates a sparse matrix for memory efficiency. Check `bag_of_words.shape`. You should see 4000 vectors, each with a dimension of 20719."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 20719)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bag_of_words is now a 4000 X 20,719 feature matrix, where every row is a move review, and every column is the count of words for the word that column represents. The words can be found using the `.get_feature_names()` method.\n",
    "\n",
    "Check the 200th word in `count_vect`. It should be `adulthood`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adulthood'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names()[200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we can do the opposite using `.vocabulary_.get()`. Check `'adulthood'`; it should be the 200th word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get('adulthood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "In Python, the `TfidfVectorizer` object represents the Bag Of Words model. import it from `sklearn.feature_extraction.text`, and create a `CountVectorizer()` object called `tf_idf_vect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_idf_vect = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, just like you did with the `CountVectorizer`, fit and transform your clean text reviews and store the result in a variable called `tf_idf`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_idf = tf_idf_vect.fit_transform(df_reviews['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print `tf_idf` and notice how the values are different. Print the `shape` to confirm that the dimensions are the same as `bag_of_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2278)\t0.585588780724\n",
      "  (0, 8284)\t0.265921160639\n",
      "  (0, 2762)\t0.0902999132203\n",
      "  (0, 3537)\t0.0563874590305\n",
      "  (0, 14695)\t0.0945774478549\n",
      "  (0, 18420)\t0.032304584135\n",
      "  (0, 14289)\t0.0916098400766\n",
      "  (0, 15898)\t0.204555821375\n",
      "  (0, 10470)\t0.0473356073499\n",
      "  (0, 18066)\t0.361199652881\n",
      "  (0, 20542)\t0.0443336898392\n",
      "  (0, 18065)\t0.0906180654526\n",
      "  (0, 14274)\t0.109971880498\n",
      "  (0, 10300)\t0.0568227934732\n",
      "  (0, 1562)\t0.0509203455254\n",
      "  (0, 15781)\t0.0949876038184\n",
      "  (0, 3377)\t0.0981740657368\n",
      "  (0, 14793)\t0.0775818392717\n",
      "  (0, 15956)\t0.127698101194\n",
      "  (0, 17782)\t0.0858828642895\n",
      "  (0, 6429)\t0.105583461765\n",
      "  (0, 9067)\t0.0906180654526\n",
      "  (0, 17536)\t0.312759136853\n",
      "  (0, 15279)\t0.0536901910038\n",
      "  (0, 13360)\t0.0876761686291\n",
      "  :\t:\n",
      "  (3999, 19905)\t0.0916471906954\n",
      "  (3999, 12051)\t0.1126913754\n",
      "  (3999, 18145)\t0.220035597446\n",
      "  (3999, 2181)\t0.0854704855585\n",
      "  (3999, 8399)\t0.0939076675673\n",
      "  (3999, 9101)\t0.101129378855\n",
      "  (3999, 16251)\t0.0986796855085\n",
      "  (3999, 20030)\t0.104072028647\n",
      "  (3999, 14070)\t0.0956348466385\n",
      "  (3999, 14343)\t0.120182159803\n",
      "  (3999, 10388)\t0.115963568798\n",
      "  (3999, 11260)\t0.0986796855085\n",
      "  (3999, 1737)\t0.107757321851\n",
      "  (3999, 2627)\t0.0975929607709\n",
      "  (3999, 7984)\t0.0931148218702\n",
      "  (3999, 16167)\t0.120182159803\n",
      "  (3999, 15580)\t0.104072028647\n",
      "  (3999, 2384)\t0.120182159803\n",
      "  (3999, 14091)\t0.126127929878\n",
      "  (3999, 5970)\t0.126127929878\n",
      "  (3999, 11414)\t0.378383789635\n",
      "  (3999, 7988)\t0.126127929878\n",
      "  (3999, 10987)\t0.252255859757\n",
      "  (3999, 7662)\t0.126127929878\n",
      "  (3999, 16866)\t0.126127929878\n"
     ]
    }
   ],
   "source": [
    "tf_idf.shape\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, because our dataset has so many unique words, tfidf vectorizer creates a sparse matrix.\n",
    "\n",
    "This matrix will again be 4000 X 20,719, where each column is the term frequence (count of times that word appears in the review) times the by the inverse document frequency (basically total number of reviews / number of reviews the word appears in)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the differences in the two feature sets for one of our reviews.\n",
    "\n",
    "Print the 9084th word in any one of our objects (`count_vect` or `tf_idf_vect`). It should be `inspir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inspir'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vect.get_feature_names()[9084]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how often it appears in Review 1: print the value of `(1, 9084)` in your `bag_of_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words[(1, 9084)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see `1` !\n",
    "\n",
    "What about it's tf-idf value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.045896271833222251"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf[(1, 9084)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see `0.04589627183322225`.\n",
    "\n",
    "Notice how much smaller it is? This means it must appear in a good deal of other reviews\n",
    "\n",
    "### Classification\n",
    "\n",
    "Now, let's make a classifier to actually feed our feature data and train/test it. We'll use a Logistic Regression Classifier.\n",
    "\n",
    "First, do this for the `CountVectorizer`. Use `train_test_split` with `test_size=0.1` and `random_state=42`. Your features will simply be your `bag_of_words` and your labels will be `df_reviews['label']`. Because our data is balanced, you can use `accuracy_score` if you like to check the accuracy of your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(bag_of_words, df_reviews['label'], test_size=0.1, random_state=42)\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "y_predict = lr.predict(X_test)\n",
    "print(accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "88.75 %, not bad! \n",
    "\n",
    "Now let's try using our `TfidfVectorizer` and see if it performs better. Use the same parameters as above, the only different is that your features are `tf_idf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf, df_reviews['label'], test_size=0.1, random_state=9)\n",
    "lr_tf = LogisticRegression().fit(X_train, y_train)\n",
    "y_predict = lr_tf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90.25% ! So in this case, tf-idf is a little more accurate than bag of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Doc2Vec` documentation can be found here:<br>\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "A readable, easy introduction to `Doc2Vec` is available in this medium article:<br>\n",
    "https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e\n",
    "\n",
    "You don't need to understand the main details about how `Doc2Vec` works, but it's more important that you understand how to use it -- and *that* will be the goal of this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First of all, you need to install `gensim`, which is the module that contains `Doc2Vec`. Open up your Terminal (on Mac) or Command Prompt (on Windows) and type in the following:\n",
    "\n",
    "`easy_install -U gensim`\n",
    "\n",
    "### Modules\n",
    "\n",
    "We use `gensim`, since `gensim` has a much more readable implementation of `Word2Vec` (and `Doc2Vec`). We also use `numpy` for general array manipulation, and `sklearn` for Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, from `gensim.models` import `Doc2Vec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, import the usual suspects: `numpy`, and `LogisticRegression` from `sklearn.linear_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Doc2Vec model\n",
    "\n",
    "The way Doc2Vec works is that, each 'document' (lyrics of a song, words in an email, etc.) needs to be fully 'cleaned' (no punctuation, stemmed, etc.) and on a single line each in a `.txt` file. In our case, we have 50,000 movie reviews, split into 4 different `.txt` files:\n",
    "\n",
    "- `test-neg.txt`: 12500 negative movie reviews from the test data\n",
    "- `test-pos.txt`: 12500 positive movie reviews from the test data\n",
    "- `train-neg.txt`: 12500 negative movie reviews from the training data\n",
    "- `train-pos.txt`: 12500 positive movie reviews from the training data\n",
    "\n",
    "#### Check out the above text files and briefly go through them.\n",
    "\n",
    "You can look at the `Doc2VecHelperFunctions.ipynb` file if you are curious to see how the text files are converted into our Doc2Vec model, `imdb.d2v`.\n",
    "\n",
    "If you're curious about the parameters, do read the Doc2Vec/Word2Vec [documentation](\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, the model is already prepared. It is named `imdb.d2v`. Load it by calling `Doc2Vec.load('./imdb.d2v')` and save it in a variable called `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('imdb.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Model\n",
    "\n",
    "Let's see what our model gives. If we want to see what words are most 'similar' to `'good'`, we can call `model.vw.most_similar('good')` on our model. Try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decent', 0.7383522391319275),\n",
       " ('great', 0.7068538665771484),\n",
       " ('bad', 0.6740086078643799),\n",
       " ('fine', 0.6538171768188477),\n",
       " ('solid', 0.6522965431213379),\n",
       " ('nice', 0.6312328577041626),\n",
       " ('excellent', 0.58672696352005),\n",
       " ('terrific', 0.5646074414253235),\n",
       " ('poor', 0.5573974847793579),\n",
       " ('strong', 0.5290022492408752)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are some of the words above used in similar ways in which you would use the word 'good' ? If yes, that means our model has kind of understood the *meaning* of the word `good`. This is really awesome (and important), since we are doing sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look deeper and see what the model actually contains. To see the feature vector for the first review in the training set for negative reviews, check `model['TRAIN_NEG_0']`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15591073, -1.00769353, -0.29605961,  2.18465304,  0.9532637 ,\n",
       "       -3.87005091, -2.33878136, -1.34991467,  1.09373116,  2.16895318,\n",
       "       -0.15583986, -0.20290834,  0.96698886,  2.25928164,  0.38636756,\n",
       "        1.23074746,  1.31522524,  0.01632519,  2.00626898, -2.68454742,\n",
       "        0.7009055 ,  1.54163992, -0.50989616, -0.36360747, -0.65092176,\n",
       "        0.82744628,  1.19471276, -0.33523622, -1.06338334, -0.53705174,\n",
       "       -0.6888628 ,  1.54129934, -0.77872807, -0.26769787,  0.11388908,\n",
       "       -2.11309719,  3.09696221,  0.94127595,  1.019858  ,  1.09815776,\n",
       "       -0.19643727,  2.81080961, -1.01270771, -1.69791448, -0.71819431,\n",
       "        0.82034248,  1.07061815, -0.72113127,  0.57014245, -2.12918043,\n",
       "        1.90972507,  3.25470948, -0.14005719,  0.7353341 , -1.21092224,\n",
       "       -1.59447384,  1.04394937, -2.00083256,  3.26442623,  0.95649093,\n",
       "       -0.29830626, -1.43709934,  0.14037158,  0.28651956, -0.08995567,\n",
       "        0.83530617, -0.67329133,  0.7457509 ,  1.15693736, -2.04809022,\n",
       "        0.19059812, -0.58927757,  0.63839155, -1.37294424, -0.52826262,\n",
       "        0.5157848 , -0.30142826,  1.04464245, -1.32536614, -0.34307244,\n",
       "        2.15757704,  1.68040693, -1.81541681, -1.14183855, -0.6072163 ,\n",
       "       -2.52647591, -0.18598452,  2.84770894,  2.85956216, -0.9046604 ,\n",
       "        0.90220517,  1.30166376,  0.40313169, -3.36989808,  0.41899014,\n",
       "       -0.26486805,  0.25631961, -1.45913517,  1.49660051,  1.72079444], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['TRAIN_NEG_2']\n",
    "model['TRAIN_POS_1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.doc2vec.Doc2Vec"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Sentiments\n",
    "\n",
    "### Training Vectors\n",
    "\n",
    "Now let's use these vectors to train a classifier. First, we must extract the training vectors. Remember that we have a total of 25000 training reviews, with equal numbers of positive and negative ones (12500 positive, 12500 negative). There are two parallel arrays, one containing the vectors (`train_arrays`) and the other containing the labels (`train_labels`). We simply put the positive ones at the first half of the array, and the negative ones at the second half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a `for` loop to go through all `25000` training reviews, adding the vector for each review in `train_arrays` and it's corresponding label (`1` for a positive review, and `0` for a negative review) in `train_labels`.\n",
    "\n",
    "#### Read the code below and ask your Instructor/TA if you have any questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arrays = np.zeros((25000, 100))\n",
    "train_labels = np.zeros(25000)\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_train_pos = 'TRAIN_POS_' + str(i)\n",
    "    prefix_train_neg = 'TRAIN_NEG_' + str(i)\n",
    "    train_arrays[i] = model[prefix_train_pos]\n",
    "    train_arrays[12500 + i] = model[prefix_train_neg]\n",
    "    train_labels[i] = 1\n",
    "    train_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print `train_arrays`. You should see rows and rows of vectors representing each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.09523074  0.10516991 -0.07066526 ..., -1.50765908  0.37817046\n",
      "   0.45435163]\n",
      " [ 0.15591073 -1.00769353 -0.29605961 ..., -1.45913517  1.49660051\n",
      "   1.72079444]\n",
      " [-0.49689472 -0.63923281 -1.31833351 ..., -2.12929225  0.9443326\n",
      "   0.63289094]\n",
      " ..., \n",
      " [-0.2536512  -0.89831948 -0.24197805 ...,  1.50290143  1.01230037\n",
      "  -0.3398996 ]\n",
      " [-2.00854516  0.64646685 -0.45022076 ...,  1.535079    0.13337763\n",
      "   0.06628666]\n",
      " [-0.50857788  0.85919422 -0.78979629 ..., -0.4446539   1.05848455\n",
      "   0.50058913]]\n"
     ]
    }
   ],
   "source": [
    "print(train_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print `train_labels`. They are simply category labels for the sentence vectors -- 1 representing positive and 0 for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Vectors\n",
    "\n",
    "We do the same for testing data -- data that we are going to feed to the classifier after we've trained it using the training data. This allows us to evaluate our results. The process is pretty much the same as extracting the results for the training data.\n",
    "\n",
    "#### Read the code below and ask your Instructor/TA if you have any questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arrays = np.zeros((25000, 100))\n",
    "test_labels = np.zeros(25000)\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_test_pos = 'TEST_POS_' + str(i)\n",
    "    prefix_test_neg = 'TEST_NEG_' + str(i)\n",
    "    test_arrays[i] = model[prefix_test_pos]\n",
    "    test_arrays[12500 + i] = model[prefix_test_neg]\n",
    "    test_labels[i] = 1\n",
    "    test_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Now, train a logistic regression classifier using the training data.\n",
    "\n",
    "Create a LogisticRegression Classifier, and `fit` it to your `train_arrays` and `train_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_doc2vec = LogisticRegression().fit(train_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `score` on your classifier, passing in your `test_arrays` and `test_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86439999999999995"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = accuracy_score(test_labels, lr_doc2vec.predict(test_arrays))\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that we have achieved nearly 87% accuracy for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you have time, try running your classifier on a bunch of individual reviews and see if you agree with the predictions! You can do this in the following steps:\n",
    "* Choose a review from one of the `.txt` files.\n",
    "* You can grab it's corresponding vector by using the correct index in your model.\n",
    "    * For example, for the 3rd negative test review, the feature vector is `model['TEST_NEG_2']`\n",
    "* Call `classifier.predict` on your feature vector to see the prediction (you may have to use `.reshape` to get it in the correct format).\n",
    "    * A result of 0 means it's a positive review, and 1 means negative.\n",
    "* Do you agree :) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try a random review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try a random review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try a random review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try a random review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try a random review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, ask your Instructor or TA if you have any questions. Good luck on the Assignment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Doc2vec: https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "- Paper that inspired this: https://arxiv.org/pdf/1405.4053.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
